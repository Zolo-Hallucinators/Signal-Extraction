{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "ww6kgwpovoyptdas2e5z",
   "authorId": "3911389675025",
   "authorName": "ARAVINDSURESH",
   "authorEmail": "aravind.2002.suresh@gmail.com",
   "sessionId": "3b8d164e-b6b3-4d8f-aa4b-f48c32734848",
   "lastEditTime": 1757677762020
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e71084-5d94-4374-b69b-01cd1fad938f",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# NEWS API Ingestion (Dynamic)"
  },
  {
   "cell_type": "markdown",
   "id": "8e9259c8-e6f0-4952-9902-8404b53498b7",
   "metadata": {
    "name": "ConfigSetupMD",
    "collapsed": false
   },
   "source": "## Runtime Config Set-Up"
  },
  {
   "cell_type": "code",
   "id": "7ffbfec7-1c19-4e66-bd1c-f6bfef55f3c0",
   "metadata": {
    "language": "python",
    "name": "ConfigSetUp"
   },
   "outputs": [],
   "source": "CONFIG_PATH = \"config.json\"\nRUN_NAME = \"PalantirTestRun\" #<TODO: Make Dynamic>",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "634f3090-c61e-4696-a576-14ae691d1435",
   "metadata": {
    "name": "ImportLibMD",
    "collapsed": false
   },
   "source": "### Importing Libraries"
  },
  {
   "cell_type": "code",
   "id": "8d1e36b5-4ba8-4b9e-b2d6-05d662d8ff14",
   "metadata": {
    "language": "python",
    "name": "ImportLib"
   },
   "outputs": [],
   "source": "import requests\nimport json\nimport pandas as pd\nfrom datetime import date, timedelta, datetime\nimport math\nimport warnings\nfrom bs4 import BeautifulSoup\nimport time\nimport snowflake.connector\nfrom urllib.parse import urlparse\nimport hashlib\nfrom snowflake.connector.pandas_tools import write_pandas",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "840fb21b-4c15-4861-a795-914b47a6e9b5",
   "metadata": {
    "name": "EndpointTestMD",
    "collapsed": false
   },
   "source": "### Testing Endpoint"
  },
  {
   "cell_type": "code",
   "id": "6c158b0c-1942-4e37-a6ac-b2d8f9289c4e",
   "metadata": {
    "language": "python",
    "name": "EndpointTest"
   },
   "outputs": [],
   "source": "resp = requests.get('https://newsapi.org/v2/everything?q=apple&to=2025-09-10&from=2025-09-10&apiKey=c494f280427646c78473013990b3cd45')\nlen(resp.json())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e6212bf-57f4-4097-af51-49b50d2b728b",
   "metadata": {
    "name": "MainCodeMD",
    "collapsed": false
   },
   "source": "## Main Code"
  },
  {
   "cell_type": "code",
   "id": "7d68cb81-c365-40fe-a5d3-af1aea7d0735",
   "metadata": {
    "language": "python",
    "name": "ConfigLoad",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def get_config(CONFIG_PATH=\"config.json\"):\n    with open(CONFIG_PATH) as f:\n        config = json.load(f)\n    return config\n\ndef get_config_run(RUN_NAME=\"PalantirTestRun\"):\n    config = get_config()\n    config_run = next((r for r in config['runs'] if r['run_name'] == RUN_NAME), None)\n    return config_run\n\ndef get_config_snowflake():\n    config = get_config()\n    config_snowflake = config[\"snowflake\"]\n    return config_snowflake\n\ndef get_snowlfake_conn(schema_name):\n    config_snowflake = get_config_snowflake()\n    conn = snowflake.connector.connect(\n        user=config_snowflake[\"user\"],\n        password=config_snowflake[\"password\"],\n        account=config_snowflake[\"account\"],\n        warehouse=config_snowflake[\"warehouse\"],\n        database=config_snowflake[\"database\"],\n        schema=schema_name # SQL Specific\n    )\n    return conn\n    \nconfig = get_config() #DEBUG\nconfig_run = get_config_run()\nconfig_snowflake = get_config_snowflake()\nprint(config, config_run, config_snowflake)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a31cbd32-6636-4312-9902-7e2169e1ab38",
   "metadata": {
    "language": "python",
    "name": "URLString",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def create_url_string(config_run, config):\n    calc_to = date.today()\n    calc_from = calc_to - timedelta(days=config_run[\"from_days_ago\"])\n    calc_from = calc_from.isoformat()\n    \n    url_string = f\"\"\"\n        {config_run[\"endpoint\"]}?\n        q={config_run[\"q\"]}&\n        language={config_run[\"language\"]}&\n        to={calc_to}&\n        from={calc_from}&\n        sortBy={config_run[\"sortBy\"]}&\n        apiKey={config[\"news_api_key\"]}\n    \"\"\"\n    url_string = url_string.replace(' ', '').replace('\\n', '')\n    return url_string\n\nconfig_run = get_config_run(RUN_NAME) #RUN_NAME from global\nurl_string = create_url_string(config_run, config)\nprint(url_string)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6feb7410-9596-47e3-b57f-deed0d937a43",
   "metadata": {
    "language": "python",
    "name": "ToFetchNews",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "def fetch_news(url_string):\n    \"\"\"\n    Fetches news, makes sures all the pages are scraped\n    \"\"\"\n    response = requests.get(url_string, params = {\"page\": 1})\n    data = response.json()\n\n    total_results = data.get(\"totalResults\", 0)\n    articles_data = data.get(\"articles\", [])\n    # print(f'total_results: {total_results} | articles_data: {articles_data}')\n    \n    if total_results == 0:\n        return articles_data\n        \n    if total_results > 100:\n        total_pages = math.ceil(total_results / 100)\n        for page in range(2, total_pages+1):\n            print(f'Going through page {page}/{total_pages}')\n            response = requests.get(url_string, params = {\"page\": page})\n            data = response.json()\n            \n            page_articles = data.get(\"articles\")\n            articles_data.extend(page_articles)\n            # print(page_articles)\n            \n    if len(articles_data) != total_results:\n        warnings.warn(f'Article Count Mismatch\" {len(articles_data)}|{total_results}')\n\n    print(f'No. of Articles: ')\n    return articles_data\n\nprint(url_string)\narticles_data = fetch_news(url_string)\nprint(articles_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "632b0a14-47c1-4204-aa07-dc2cb8872798",
   "metadata": {
    "language": "python",
    "name": "CreatinbBaseDF",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def articles_to_df(articles_data):\n    def get_domain(url: str) -> str:\n        \"\"\"Extract domain from URL (finance.yahoo.com, etc.).\"\"\"\n        if not url or pd.isna(url):\n            return None\n        parsed = urlparse(url)\n        return parsed.netloc.lower()\n        \n    df = pd.json_normalize(articles_data, sep=\"_\") #Flatten and separate by\n    column_rename_mapping = {\n        \"urlToImage\": \"url_to_image\",\n        \"publishedAt\": \"published_at_utc\",\n        \"content\": \"content_truncated\"\n    }\n    df = df.rename(columns=column_rename_mapping)\n    df[\"url_domain\"] = df[\"url\"].apply(get_domain)\n    df[\"published_at_utc\"] = pd.to_datetime(df[\"published_at_utc\"], utc=True).dt.tz_localize(None)\n    return df\n\ndf = articles_to_df(articles_data)\ndf.head(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc8028a3-bb17-4926-a3c5-27f1f1145f8a",
   "metadata": {
    "language": "python",
    "name": "CreateNR",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def ensure_network_rule_for_domain(\n    df, \n    nr_name = \"news_domains_nr\", \n    integration_name = \"news_domains_integration\"):\n    \"\"\"\n    Create or replace a single Snowflake network rule that covers all given domains.\n    Attach it to the API integration.\n    \"\"\"\n    conn = get_snowlfake_conn(schema_name=\"UTILS\")\n    cur = conn.cursor() #Start\n\n    cur.execute(f\"DESCRIBE NETWORK RULE {nr_name}\")\n    desc_rows = cur.fetchall()\n    desc_list = list(desc_rows[0])\n    \n    for idx, val in enumerate(desc_list):\n        if \".com\" in str(val).lower():\n            value_list_str_existing = val.lower()\n    print(value_list_str_existing)\n            \n    value_list_existing = [d.strip().strip(\"'\").lower() for d in value_list_str_existing.split(\",\")]\n    value_list_latest = [i.lower() for i in list(df['url_domain'].unique())]\n    value_list_optimised = list(set(value_list_existing + value_list_latest))\n    value_list_str_optimised = \", \".join([f\"'{d}'\" for d in value_list_optimised])\n    print(f'Existing Value List: {value_list_existing}')\n    print(f'Latest Value List: {value_list_latest}')\n    print(f'Optimised Value List: {value_list_optimised}')\n    print(f'NR String: {value_list_str_optimised}')\n\n    # Network Rule SQL\n    create_sql = f\"\"\"\n    CREATE OR REPLACE NETWORK RULE {nr_name}\n        TYPE = HOST_PORT\n        MODE = EGRESS\n        VALUE_LIST = ({value_list_str_optimised});\n    \"\"\"\n    cur.execute(create_sql)\n    \n    # Collect all network rules and re-attach to integration\n    alter_sql = f\"\"\"\n    ALTER EXTERNAL ACCESS INTEGRATION {integration_name}\n        SET ALLOWED_NETWORK_RULES = ({nr_name})\n        ENABLED = TRUE;\n    \"\"\"\n    cur.execute(alter_sql)\n    cur.close() #End\n    \n    print(f\"Network rule ({nr_name}) updated in Integration ({integration_name})\")\n\nensure_network_rule_for_domain(df)\n    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2dbe2028-464a-4404-bb46-10279c649b1f",
   "metadata": {
    "language": "python",
    "name": "Scrape",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def scrape_url_helper(url):\n    try:\n        response = requests.get(url, timeout=10)\n        # if response.status_code != 200:\n        #     return None, 0\n            \n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # title = soup.title.string.strip() if soup.title else \"\" # Extract title\n        paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')] # Extract all paragraph text\n        content = \" \".join(paragraphs)\n        \n        # paragraphs = soup.find_all(\"p\") #[GPT Method] Keeps formatting intact (other method, removes additional newlines and spaces)\n        # content = \" \".join([p.get_text() for p in paragraphs if p.get_text()])\n        # content = content.strip()\n        print(f'{url} - content')\n        return content, len(content)\n    except requests.exceptions.Timeout as e:\n        # Handle connection timeout specifically\n        print(f\"TIMEOUT for url {url}: {e}\")\n        return None, -2\n    except Exception as e:\n        # If error (broken link, paywall, etc.)\n        print(f\"ERROR for url {url}: {type(e).__name__} â€” {e}\")\n        return None, -1 #(-1 to indicate that domain network rule not created)\n\ndef scrape_url(df, chunk_size=30, delay_between_chunks=3):\n    df['content_full'] = None\n    df['content_size'] = 0\n    df_len = len(df)\n\n    for idx, row in df.iterrows():\n        url = row.get('url')\n        if not url:\n            continue\n        content_full, content_size = scrape_url_helper(url)\n        print(f'{idx+1}/{df_len} | Scraped url({content_size}): {url}')\n        df.at[idx, 'content_full'] = content_full\n        df.at[idx, 'content_size'] = content_size\n        if idx%chunk_size == 0:\n            print(f'{idx+1}/{df_len} | Sleeping for {delay_between_chunks}s')\n            time.sleep(delay_between_chunks)\n    return df\n    \n\ndf = scrape_url(df)\ndf.head(20)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35583064-8829-4510-b24f-230743eacd76",
   "metadata": {
    "name": "UpdatingSchemaMD",
    "collapsed": false
   },
   "source": "## Pushing to RAW Schema"
  },
  {
   "cell_type": "code",
   "id": "46c72de0-c6b8-449e-b669-3d29dd2dfcc2",
   "metadata": {
    "language": "python",
    "name": "PreprocessDF",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def preprocess_dataframe(df):\n    \"\"\"\n    Creating the primary key hash & ingested_at value.\n    \"\"\"\n    df = df.copy()\n    df[\"article_id\"] = df.apply(\n        lambda row: hashlib.sha1(f\"{row['url']}_{row['published_at_utc']}\".encode()).hexdigest(),\n        axis=1\n    )\n    # \"ingested_at\" is re-written when pushing to prod raw using SQL\n    df[\"ingested_at\"] = pd.Timestamp.now(tz=\"UTC\").tz_localize(None)\n    config_run = get_config_run(RUN_NAME)\n    # df[\"entity_name\"] = config_run[\"entity_name\"]\n    expected_cols = [\n        \"article_id\",\"author\",\"title\",\"description\",\"url\",\"url_to_image\",\n        \"published_at_utc\",\"content_truncated\",\"source_id\",\"source_name\",\n        \"url_domain\",\"content_full\",\"content_size\",\"ingested_at\"\n    ]\n    return df[expected_cols]\n\ndf = preprocess_dataframe(df)\ndf.head(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d11bad1-e553-4337-a637-03325f5326e8",
   "metadata": {
    "language": "python",
    "name": "CreateTempTable",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Creating a Temp view & table - Session Scoped\ndef create_temp_table_from_df(df, temp_table=\"NEWS_ARTICLES_TEMP\"):\n    \"\"\"\n    Creating a Temp Table (Session Scoped), can be used within the notebook.\n    \"\"\"\n\n    conn = get_snowlfake_conn(schema_name=\"RAW\")\n    cur = conn.cursor()\n\n    try:\n        # Drop old temp table if exists\n        cur.execute(f\"DROP TABLE IF EXISTS {temp_table}\")\n\n        # Write dataframe to a temp table\n        success, nchunks, nrows, _ = write_pandas(\n            conn, \n            df, \n            table_name=temp_table, \n            auto_create_table=True, \n            overwrite=True, \n            quote_identifiers=False,\n            use_logical_type=True\n        )\n\n        print(f\"[INFO] Temp table created: {temp_table}, Rows inserted: {nrows}\")\n    \n    finally:\n        cur.close()\n        conn.close()\n    \ncreate_temp_table_from_df(df, temp_table=\"NEWS_ARTICLES_TEMP\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cd5d770-c69b-4732-ab0c-e5277b0b77f4",
   "metadata": {
    "language": "sql",
    "name": "Test",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM NEWS_ARTICLES_TEMP LIMIT 3;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d0d5027-cd66-42b5-87e2-20a425220a94",
   "metadata": {
    "language": "sql",
    "name": "MainMergeQuery",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "MERGE INTO SIGNAL_EXTRACTION_DB.RAW.NEWS_ARTICLES AS target\nUSING NEWS_ARTICLES_TEMP AS source\nON target.article_id = source.article_id\nWHEN MATCHED THEN UPDATE SET\n    -- target.entity_name       = source.entity_name,\n    target.author            = source.author,\n    target.title             = source.title,\n    target.description       = source.description,\n    target.url               = source.url,\n    target.url_to_image      = source.url_to_image,\n    target.published_at_utc  = source.published_at_utc,\n    target.content_truncated = source.content_truncated,\n    target.source_id         = source.source_id,\n    target.source_name       = source.source_name,\n    target.url_domain        = source.url_domain,\n    target.content_full      = source.content_full,\n    target.content_size      = source.content_size,\n    target.ingested_at       = CURRENT_TIMESTAMP\nWHEN NOT MATCHED THEN\n    INSERT (\n        -- entity_name,\n        article_id, author, title, description, url,\n        url_to_image,published_at_utc, content_truncated,\n        source_id, source_name, url_domain, \n        content_full, content_size, ingested_at\n    )\n    VALUES (\n        -- source.entity_name,\n        source.article_id, source.author, source.title, \n        source.description, source.url,\n        source.url_to_image, source.published_at_utc,\n        source.content_truncated, source.source_id,\n        source.source_name, source.url_domain,\n        source.content_full, source.content_size, CURRENT_TIMESTAMP\n    );\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e66afbcc-44c7-491c-b4b6-1b31fc25c2e3",
   "metadata": {
    "language": "sql",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM SIGNAL_EXTRACTION_DB.RAW.NEWS_ARTICLES LIMIT 3;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b6a4ef3-91d4-49b6-abec-e77e5a45a3b0",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "insert_sql = \"\"\"\nINSERT INTO RAW.ARTICLE_ENTITY_MAP (article_id, q_value, entity_name)\nSELECT s.article_id, s.q_value, s.entity_name\nFROM NEWS_ARTICLES_TEMP s\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM RAW.ARTICLE_ENTITY_MAP m\n    WHERE m.article_id  = s.article_id\n      AND m.q_value     = s.q_value\n      AND m.entity_name = s.entity_name\n);\n\"\"\"",
   "execution_count": null
  }
 ]
}