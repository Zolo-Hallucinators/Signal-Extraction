{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "56a3om2vl5iunkalek5h",
   "authorId": "65353703147",
   "authorName": "ARAVINDSURESH03",
   "authorEmail": "aravind.ofcl8@gmail.com",
   "sessionId": "4feac5e2-58bb-4f70-bb64-f505eb46c04d",
   "lastEditTime": 1759596153991
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e71084-5d94-4374-b69b-01cd1fad938f",
   "metadata": {
    "name": "TitleCell",
    "collapsed": false
   },
   "source": "# NEWS API Ingestion (Dynamic)"
  },
  {
   "cell_type": "markdown",
   "id": "d6f05a14-ed7b-4308-bc77-3e8172b24e46",
   "metadata": {
    "name": "PreRunActivity",
    "collapsed": false
   },
   "source": "### Pre Run Activity\n- Make sure to tap the 3 dot icon on top left and enable the necessary 'External Access' integrations for the notebook.\n- Update the 'config.json' with the requirement before starting the run."
  },
  {
   "cell_type": "markdown",
   "id": "8e9259c8-e6f0-4952-9902-8404b53498b7",
   "metadata": {
    "name": "ConfigSetupMD",
    "collapsed": false
   },
   "source": "## Runtime Config Set-Up"
  },
  {
   "cell_type": "code",
   "id": "7ffbfec7-1c19-4e66-bd1c-f6bfef55f3c0",
   "metadata": {
    "language": "python",
    "name": "ConfigSetUp"
   },
   "outputs": [],
   "source": "GLOBAL_CONFIG_PATH = \"config.json\"",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "634f3090-c61e-4696-a576-14ae691d1435",
   "metadata": {
    "name": "ImportLibMD",
    "collapsed": false
   },
   "source": "### Importing Libraries"
  },
  {
   "cell_type": "code",
   "id": "8d1e36b5-4ba8-4b9e-b2d6-05d662d8ff14",
   "metadata": {
    "language": "python",
    "name": "ImportLib"
   },
   "outputs": [],
   "source": "import requests\nimport json\nimport pandas as pd\nfrom datetime import date, timedelta, datetime\nimport math\nimport warnings\nfrom bs4 import BeautifulSoup\nimport time\nimport random\nimport snowflake.connector\nfrom urllib.parse import urlparse\nimport hashlib\nfrom snowflake.connector.pandas_tools import write_pandas",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "809c73e6-da10-4164-b22d-d7e6b39c0266",
   "metadata": {
    "name": "RunAutoPickupMD",
    "collapsed": false
   },
   "source": "### Auto Run-Name Initiationa Code"
  },
  {
   "cell_type": "code",
   "id": "c0efd7b7-cc45-43ab-b07e-cf35772a45c7",
   "metadata": {
    "language": "python",
    "name": "RunAutoPickup"
   },
   "outputs": [],
   "source": "def get_config(CONFIG_PATH=\"config.json\"):\n    with open(CONFIG_PATH) as f:\n        config = json.load(f)\n    return config\nconfig = get_config(GLOBAL_CONFIG_PATH) #DEBUG\n\nGLOBAL_RUN_NAME = config['run_auto_pickup']",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "840fb21b-4c15-4861-a795-914b47a6e9b5",
   "metadata": {
    "name": "EndpointTestMD",
    "collapsed": false
   },
   "source": "### Testing Endpoint"
  },
  {
   "cell_type": "code",
   "id": "6c158b0c-1942-4e37-a6ac-b2d8f9289c4e",
   "metadata": {
    "language": "python",
    "name": "EndpointTest"
   },
   "outputs": [],
   "source": "resp = requests.get('https://newsapi.org/v2/everything?q=apple&to=2025-09-10&from=2025-09-10&apiKey=c494f280427646c78473013990b3cd45')\nlen(resp.json())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e6212bf-57f4-4097-af51-49b50d2b728b",
   "metadata": {
    "name": "MainCodeMD",
    "collapsed": false
   },
   "source": "## Main Code"
  },
  {
   "cell_type": "code",
   "id": "7d68cb81-c365-40fe-a5d3-af1aea7d0735",
   "metadata": {
    "language": "python",
    "name": "ConfigLoad",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def get_config_run(run_name):\n    config = get_config()\n    config_run = next((r for r in config['runs'] if r['run_name'] == run_name), None)\n    return config_run\n\ndef get_config_snowflake():\n    config = get_config()\n    config_snowflake = config[\"snowflake\"]\n    return config_snowflake\n\ndef get_snowlfake_conn(schema_name):\n    config_snowflake = get_config_snowflake()\n    conn = snowflake.connector.connect(\n        user=config_snowflake[\"user\"],\n        password=config_snowflake[\"password\"],\n        account=config_snowflake[\"account\"],\n        warehouse=config_snowflake[\"warehouse\"],\n        database=config_snowflake[\"database\"],\n        schema=schema_name # SQL Specific\n    )\n    return conn\n    \nconfig = get_config() #DEBUG\nconfig_run = get_config_run(GLOBAL_RUN_NAME)\nconfig_snowflake = get_config_snowflake()\nprint(config, config_run, config_snowflake)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a31cbd32-6636-4312-9902-7e2169e1ab38",
   "metadata": {
    "language": "python",
    "name": "URLString",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def create_url_string(config_run, config):\n    # calc_to = date.today() #This part will get auto-optimised when set to recurring.\n    calc_to = date.today() + timedelta(days=config_run[\"till_days\"]) #If negative then the range will reduce.\n    calc_from = calc_to - timedelta(days=config_run[\"from_days_ago\"])\n    calc_from = calc_from.isoformat()\n    \n    url_string = f\"\"\"\n        {config_run[\"endpoint\"]}?\n        q={config_run[\"q\"]}&\n        language={config_run[\"language\"]}&\n        to={calc_to}&\n        from={calc_from}&\n        sortBy={config_run[\"sortBy\"]}&\n        apiKey={config[\"news_api_key_list\"][config[\"news_api_key_auto_pickup\"]]}\n    \"\"\"\n    url_string = url_string.replace(' ', '').replace('\\n', '')\n    return url_string\n\nconfig_run = get_config_run(GLOBAL_RUN_NAME) \nurl_string = create_url_string(config_run, config)\nprint(url_string)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6feb7410-9596-47e3-b57f-deed0d937a43",
   "metadata": {
    "language": "python",
    "name": "ToFetchNews",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "def fetch_news(url_string):\n    \"\"\"\n    Fetches news, makes sures all the pages are scraped\n    \"\"\"\n    response = requests.get(url_string, params = {\"page\": 1})\n    data = response.json()\n\n    total_results = data.get(\"totalResults\", 0)\n    articles_data = data.get(\"articles\", [])\n    # print(f'total_results: {total_results} | articles_data: {articles_data}')\n    \n    print(f\"Total Results: {total_results}\")\n    if total_results == 0:\n        raise Exception(\"Exception: No articles captured in current run.\")\n        return articles_data\n        \n    if total_results > 100:\n        total_pages = math.ceil(total_results / 100)\n        for page in range(2, total_pages+1):\n            print(f'Going through page {page}/{total_pages}')\n            response = requests.get(url_string, params = {\"page\": page})\n            data = response.json()\n            \n            page_articles = data.get(\"articles\")\n            if page_articles is not None: # Avoids Nonetype Object error\n                articles_data.extend(page_articles)\n            # print(page_articles)\n            \n    if len(articles_data) != total_results:\n        warnings.warn(f'Article Count Mismatch\" {len(articles_data)}|{total_results}')\n\n    print(f'No. of Articles: {len(articles_data)}')\n    return articles_data\n\nprint(url_string)\narticles_data = fetch_news(url_string)\nprint(articles_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "632b0a14-47c1-4204-aa07-dc2cb8872798",
   "metadata": {
    "language": "python",
    "name": "CreatinbBaseDF",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def articles_to_df(articles_data):\n    def get_domain(url: str) -> str:\n        \"\"\"Extract domain from URL (finance.yahoo.com, etc.).\"\"\"\n        if not url or pd.isna(url):\n            return None\n        parsed = urlparse(url)\n        return parsed.netloc.lower()\n\n    # Main\n    df = pd.json_normalize(articles_data, sep=\"_\") #Flatten and separate by\n    column_rename_mapping = {\n        \"urlToImage\": \"url_to_image\",\n        \"publishedAt\": \"published_at_utc\",\n        \"content\": \"content_truncated\"\n    }\n    df = df.rename(columns=column_rename_mapping)\n    df[\"url_domain\"] = df[\"url\"].apply(get_domain)\n    df[\"published_at_utc\"] = pd.to_datetime(df[\"published_at_utc\"], utc=True).dt.tz_localize(None)\n    return df\n\ndf = articles_to_df(articles_data)\ndf.head(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc8028a3-bb17-4926-a3c5-27f1f1145f8a",
   "metadata": {
    "language": "python",
    "name": "CreateNR",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def test_ensure_network_rule_for_domain(\n    df, \n    nr_name = \"news_domains_nr\", \n    integration_name = \"news_domains_integration\",\n    schema = \"UTILS\"):\n    \"\"\"\n    Create or replace a single Snowflake network rule that covers all given domains.\n    Attach it to the API integration.\n    \"\"\"\n    conn = get_snowlfake_conn(schema_name=schema)\n    cur = conn.cursor() #Start\n\n    cur.execute(f\"DESCRIBE NETWORK RULE SIGNAL_EXTRACTION_DB.{schema}.{nr_name}\") #Hard Coded and saved in utils schema.\n    desc_rows = cur.fetchall()\n    desc_list = list(desc_rows[0])\n    \n    for idx, val in enumerate(desc_list):\n        if \".com\" in str(val).lower():\n            value_list_str_existing = val.lower()\n    print(value_list_str_existing)\n            \n    value_list_existing = [d.strip().strip(\"'\").lower() for d in value_list_str_existing.split(\",\")]\n    value_list_latest = [i.lower() for i in list(df['url_domain'].unique())]\n    value_list_optimised = list(set(value_list_existing + value_list_latest))\n    value_list_str_optimised = \", \".join([f\"'{d}'\" for d in value_list_optimised])\n    print(f'Existing Value List ({len(value_list_existing)}): {value_list_existing}')\n    print(f'Latest Value List ({len(value_list_latest)}): {value_list_latest}')\n    print(f'Optimised Value List ({len(value_list_optimised)}): {value_list_optimised}')\n    print(f'NR String: {value_list_str_optimised}')\n\n    # Network Rule SQL\n    create_sql = f\"\"\"\n    CREATE OR REPLACE NETWORK RULE {nr_name}\n        TYPE = HOST_PORT\n        MODE = EGRESS\n        VALUE_LIST = ({value_list_str_optimised});\n    \"\"\"\n    cur.execute(create_sql)\n    \n    # Collect all network rules and re-attach to integration (Both alter & replace sql are having caching issues on re-attaching)\n    alter_sql = f\"\"\"\n    ALTER EXTERNAL ACCESS INTEGRATION {integration_name}\n        SET ALLOWED_NETWORK_RULES = ({nr_name})\n        ENABLED = TRUE;\n    \"\"\"\n    # create or replace sql\n    replace_sql = f\"\"\"\n    CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION {integration_name}\n        ALLOWED_NETWORK_RULES = (\n            {nr_name}\n        )\n        ENABLED = TRUE;\n    \"\"\"\n    \n    # cur.execute(replace_sql)\n    cur.execute(alter_sql)\n    cur.close() #End\n    \n    print(f\"Network rule ({nr_name}) updated in Integration ({integration_name})\")\n\ntest_ensure_network_rule_for_domain(df)\n    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1d2505e-a77e-473b-925a-947c0a4fc8f6",
   "metadata": {
    "language": "python",
    "name": "NRErrorFix"
   },
   "outputs": [],
   "source": "# ERROR: Too many network rules.\n\n# def ensure_network_rules_for_domains(df, \n#                                      base_nr_name=\"news_domains_nr\", \n#                                      integration_name=\"news_domains_integration\",\n#                                      schema=\"UTILS\",\n#                                      max_domains_per_rule=50):\n#     \"\"\"\n#     Fully rerunnable, fault-tolerant function to manage Snowflake network rules.\n#     - Checks all rules in the schema to collect existing domains.\n#     - Deduplicates with new domains from df.\n#     - Splits into multiple rules if total domains exceed max_domains_per_rule.\n#     - Attaches all rules (fully qualified) back to the integration.\n#     \"\"\"\n#     conn = get_snowlfake_conn(schema_name=schema)\n#     cur = conn.cursor()\n\n#     # 1️⃣ Fetch all network rules in the schema\n#     try:\n#         cur.execute(f\"SHOW NETWORK RULES IN SCHEMA {schema}\")\n#         rules_info = cur.fetchall()\n#         existing_rule_names = [row[1] for row in rules_info]  # rule name in 2nd column\n#         print(f\"Existing rules in schema {schema}: {existing_rule_names}\")\n#     except Exception as e:\n#         print(f\"Error fetching rules in schema {schema}: {e}\")\n#         existing_rule_names = []\n\n#     # 2️⃣ Collect all existing domains from all rules\n#     existing_domains = set()\n#     for nr_name in existing_rule_names:\n#         try:\n#             cur.execute(f\"DESCRIBE NETWORK RULE {schema}.{nr_name}\")\n#             desc_rows = cur.fetchall()\n#             desc_list = list(desc_rows[0])\n#             for val in desc_list:\n#                 if \".com\" in str(val).lower():\n#                     domains = [d.strip().strip(\"'\").lower() for d in val.split(\",\")]\n#                     existing_domains.update(domains)\n#         except Exception as e:\n#             print(f\"Warning: could not read rule {nr_name}: {e}\")\n\n#     print(f\"Domains already covered: {len(existing_domains)}\")\n\n#     # 3️⃣ Collect latest unique domains from df\n#     latest_domains = set([i.lower() for i in df['url_domain'].unique()])\n\n#     # 4️⃣ Merge and deduplicate\n#     all_domains = list(existing_domains.union(latest_domains))\n#     print(f\"Total domains to cover after merge: {len(all_domains)}\")\n\n#     # 5️⃣ Split into chunks and prepare rules\n#     rules_sql_list = []\n#     rule_names = []\n#     num_chunks = (len(all_domains) + max_domains_per_rule - 1) // max_domains_per_rule\n#     for i in range(num_chunks):\n#         chunk = all_domains[i*max_domains_per_rule : (i+1)*max_domains_per_rule]\n#         nr_name = f\"{base_nr_name}_{i+1}\" if num_chunks > 1 else base_nr_name\n#         value_list_str = \", \".join([f\"'{d}'\" for d in chunk])\n#         create_sql = f\"\"\"\n#         CREATE OR REPLACE NETWORK RULE {schema}.{nr_name}\n#             TYPE = HOST_PORT\n#             MODE = EGRESS\n#             VALUE_LIST = ({value_list_str});\n#         \"\"\"\n#         rules_sql_list.append(create_sql)\n#         rule_names.append(f\"{schema}.{nr_name}\")  # fully qualified for integration\n\n#     # 6️⃣ Execute creation / update of rules\n#     for sql in rules_sql_list:\n#         try:\n#             cur.execute(sql)\n#             print(f\"Network rule executed: {sql.splitlines()[1].strip()}\")\n#         except Exception as e:\n#             print(f\"Error creating/updating network rule: {e}\")\n\n#     # 7️⃣ Attach all rules to integration\n#     allowed_rules_str = \", \".join(rule_names)\n#     alter_sql = f\"\"\"\n#     ALTER EXTERNAL ACCESS INTEGRATION {integration_name}\n#         SET ALLOWED_NETWORK_RULES = ({allowed_rules_str})\n#         ENABLED = TRUE;\n#     \"\"\"\n#     try:\n#         cur.execute(alter_sql)\n#         print(f\"Integration ({integration_name}) updated with rules: {allowed_rules_str}\")\n#     except Exception as e:\n#         print(f\"Error updating integration: {e}\")\n\n#     cur.close()\n#     print(\"Network rule update completed successfully.\")\n\n# ensure_network_rules_for_domains(df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2dbe2028-464a-4404-bb46-10279c649b1f",
   "metadata": {
    "language": "python",
    "name": "Scrape",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def scrape_url_helper(url):\n    try:\n        response = requests.get(url, timeout=10)\n        # if response.status_code != 200:\n        #     return None, 0\n            \n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # title = soup.title.string.strip() if soup.title else \"\" # Extract title\n        paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')] # Extract all paragraph text\n        content = \" \".join(paragraphs)\n        \n        # paragraphs = soup.find_all(\"p\") #[GPT Method] Keeps formatting intact (other method, removes additional newlines and spaces)\n        # content = \" \".join([p.get_text() for p in paragraphs if p.get_text()])\n        # content = content.strip()\n        print(f'{url} - content')\n        return content, len(content)\n    except requests.exceptions.Timeout as e:\n        # Handle connection timeout specifically\n        print(f\"TIMEOUT for url {url}: {e}\")\n        return None, -2\n    except Exception as e:\n        # If error (broken link, paywall, etc.)\n        print(f\"ERROR for url {url}: {type(e).__name__} — {e}\")\n        return None, -1 #(-1 to indicate that domain network rule not created)\n\ndef scrape_url(df, chunk_size, delay_between_chunks):\n    df['content_full'] = None\n    df['content_size'] = 0\n    df_len = len(df)\n\n    for idx, row in df.iterrows():\n        rand_delay_between_chunks = random.uniform(0.5, 1)\n        url = row.get('url')\n        if not url:\n            continue\n        content_full, content_size = scrape_url_helper(url)\n        print(f'{idx+1}/{df_len} | Scraped url({content_size}): {url}')\n        df.at[idx, 'content_full'] = content_full\n        df.at[idx, 'content_size'] = content_size\n        if idx%chunk_size == 0:\n            print(f'{idx+1}/{df_len} | Sleeping for {rand_delay_between_chunks}s')\n            time.sleep(rand_delay_between_chunks)\n    return df\n    \n\ndf = scrape_url(df, chunk_size=10, delay_between_chunks=3)\ndf.head(20)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35583064-8829-4510-b24f-230743eacd76",
   "metadata": {
    "name": "UpdatingSchemaMD",
    "collapsed": false
   },
   "source": "## Pushing to RAW Schema"
  },
  {
   "cell_type": "code",
   "id": "46c72de0-c6b8-449e-b669-3d29dd2dfcc2",
   "metadata": {
    "language": "python",
    "name": "PreprocessDF",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def preprocess_dataframe(df):\n    \"\"\"\n    Creating the primary key hash & ingested_at value.\n    \"\"\"\n    df = df.copy()\n    df[\"article_id\"] = df.apply(\n        lambda row: hashlib.sha1(f\"{row['url']}_{row['published_at_utc']}\".encode()).hexdigest(),\n        axis=1\n    )\n    # \"ingested_at\" is re-written when pushing to prod raw using SQL\n    df[\"ingested_at\"] = pd.Timestamp.now(tz=\"UTC\").tz_localize(None)\n    config_run = get_config_run(GLOBAL_RUN_NAME)\n    # df[\"entity_name\"] = config_run[\"entity_name\"]\n    expected_cols = [\n        \"article_id\",\"author\",\"title\",\"description\",\"url\",\"url_to_image\",\n        \"published_at_utc\",\"content_truncated\",\"source_id\",\"source_name\",\n        \"url_domain\",\"content_full\",\"content_size\",\"ingested_at\"\n    ]\n    return df[expected_cols]\n\ndf = preprocess_dataframe(df)\ndf.head(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d11bad1-e553-4337-a637-03325f5326e8",
   "metadata": {
    "language": "python",
    "name": "CreateTempTable",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Creating a Temp view & table - Session Scoped\ndef create_temp_table_from_df(df, temp_table=\"NEWS_ARTICLES_TEMP\"):\n    \"\"\"\n    Creating a Temp Table (Session Scoped), can be used within the notebook.\n    \"\"\"\n\n    conn = get_snowlfake_conn(schema_name=\"RAW\")\n    cur = conn.cursor()\n\n    try:\n        # Drop old temp table if exists\n        cur.execute(f\"DROP TABLE IF EXISTS {temp_table}\")\n\n        # Write dataframe to a temp table\n        success, nchunks, nrows, _ = write_pandas(\n            conn, \n            df, \n            table_name=temp_table, \n            auto_create_table=True, \n            overwrite=True, \n            quote_identifiers=False,\n            use_logical_type=True\n        )\n\n        print(f\"[INFO] Temp table created: {temp_table}, Rows inserted: {nrows}\")\n    \n    finally:\n        cur.close()\n        conn.close()\n    \ncreate_temp_table_from_df(df, temp_table=\"NEWS_ARTICLES_TEMP\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cd5d770-c69b-4732-ab0c-e5277b0b77f4",
   "metadata": {
    "language": "sql",
    "name": "TestQuery",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM NEWS_ARTICLES_TEMP LIMIT 3;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7575108e-2d36-4ede-93d9-13b3458d8b14",
   "metadata": {
    "language": "sql",
    "name": "MainDBTable"
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS SIGNAL_EXTRACTION_DB.RAW.NEWS_ARTICLES (\n    article_id STRING PRIMARY KEY,\n    author STRING,\n    title STRING,\n    description STRING,\n    url STRING,\n    url_to_image STRING,\n    published_at_utc TIMESTAMP_NTZ,\n    content_truncated STRING,\n    source_id STRING,\n    source_name STRING,\n    url_domain STRING,\n    content_full STRING,\n    content_size NUMBER,\n    ingested_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d0d5027-cd66-42b5-87e2-20a425220a94",
   "metadata": {
    "language": "sql",
    "name": "MainMergeQuery",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "// Still update if aritcle_id already exists, incase the latest ingestion has some updated fields or values.\n\nMERGE INTO SIGNAL_EXTRACTION_DB.RAW.NEWS_ARTICLES AS target\nUSING NEWS_ARTICLES_TEMP AS source\nON target.article_id = source.article_id\nWHEN MATCHED THEN UPDATE SET\n    -- target.entity_name       = source.entity_name,\n    target.author            = source.author,\n    target.title             = source.title,\n    target.description       = source.description,\n    target.url               = source.url,\n    target.url_to_image      = source.url_to_image,\n    target.published_at_utc  = source.published_at_utc,\n    target.content_truncated = source.content_truncated,\n    target.source_id         = source.source_id,\n    target.source_name       = source.source_name,\n    target.url_domain        = source.url_domain,\n    target.content_full      = source.content_full,\n    target.content_size      = source.content_size,\n    target.ingested_at       = CURRENT_TIMESTAMP\nWHEN NOT MATCHED THEN\n    INSERT (\n        -- entity_name,\n        article_id, author, title, description, url,\n        url_to_image,published_at_utc, content_truncated,\n        source_id, source_name, url_domain, \n        content_full, content_size, ingested_at\n    )\n    VALUES (\n        -- source.entity_name,\n        source.article_id, source.author, source.title, \n        source.description, source.url,\n        source.url_to_image, source.published_at_utc,\n        source.content_truncated, source.source_id,\n        source.source_name, source.url_domain,\n        source.content_full, source.content_size, CURRENT_TIMESTAMP\n    );\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e66afbcc-44c7-491c-b4b6-1b31fc25c2e3",
   "metadata": {
    "language": "sql",
    "name": "TestQuery2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM SIGNAL_EXTRACTION_DB.RAW.NEWS_ARTICLES LIMIT 3;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ffddb06-b4f0-4113-b1a9-bae93992e223",
   "metadata": {
    "name": "FutureDevelopMD",
    "collapsed": false
   },
   "source": "## Future Developement"
  },
  {
   "cell_type": "code",
   "id": "1b6a4ef3-91d4-49b6-abec-e77e5a45a3b0",
   "metadata": {
    "language": "sql",
    "name": "EntityMapTest",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- TODO: Future, map all the article_id s to different entities like dependent market news and others.\n\n-- insert_sql = \"\"\"\n-- INSERT INTO RAW.ARTICLE_ENTITY_MAP (article_id, q_value, entity_name)\n-- SELECT s.article_id, s.q_value, s.entity_name\n-- FROM NEWS_ARTICLES_TEMP s\n-- WHERE NOT EXISTS (\n--     SELECT 1\n--     FROM RAW.ARTICLE_ENTITY_MAP m\n--     WHERE m.article_id  = s.article_id\n--       AND m.q_value     = s.q_value\n--       AND m.entity_name = s.entity_name\n-- );\n-- \"\"\"",
   "execution_count": null
  }
 ]
}