{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "ww6kgwpovoyptdas2e5z",
   "authorId": "3911389675025",
   "authorName": "ARAVINDSURESH",
   "authorEmail": "aravind.2002.suresh@gmail.com",
   "sessionId": "31ca6ed9-172e-41cb-a97a-d4f9b644030c",
   "lastEditTime": 1757626089268
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e71084-5d94-4374-b69b-01cd1fad938f",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# NEWS API Ingestion (Dynamic)"
  },
  {
   "cell_type": "markdown",
   "id": "8e9259c8-e6f0-4952-9902-8404b53498b7",
   "metadata": {
    "name": "ConfigSetupMD",
    "collapsed": false
   },
   "source": "## Runtime Config Set-Up"
  },
  {
   "cell_type": "code",
   "id": "7ffbfec7-1c19-4e66-bd1c-f6bfef55f3c0",
   "metadata": {
    "language": "python",
    "name": "ConfigSetUp"
   },
   "outputs": [],
   "source": "CONFIG_PATH = \"config.json\"\nRUN_NAME = \"PalantirTestRun\" #<TODO: Make Dynamic>",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "634f3090-c61e-4696-a576-14ae691d1435",
   "metadata": {
    "name": "ImportLibMD",
    "collapsed": false
   },
   "source": "### Importing Libraries"
  },
  {
   "cell_type": "code",
   "id": "8d1e36b5-4ba8-4b9e-b2d6-05d662d8ff14",
   "metadata": {
    "language": "python",
    "name": "ImportLib"
   },
   "outputs": [],
   "source": "import requests\nimport json\nimport pandas as pd\nfrom datetime import date, timedelta, datetime\nimport math\nimport warnings\nfrom bs4 import BeautifulSoup\nimport time\nimport snowflake.connector\nfrom urllib.parse import urlparse",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "840fb21b-4c15-4861-a795-914b47a6e9b5",
   "metadata": {
    "name": "EndpointTestMD",
    "collapsed": false
   },
   "source": "### Testing Endpoint"
  },
  {
   "cell_type": "code",
   "id": "6c158b0c-1942-4e37-a6ac-b2d8f9289c4e",
   "metadata": {
    "language": "python",
    "name": "EndpointTest"
   },
   "outputs": [],
   "source": "resp = requests.get('https://newsapi.org/v2/everything?q=apple&to=2025-09-10&from=2025-09-10&apiKey=c494f280427646c78473013990b3cd45')\nlen(resp.json())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e6212bf-57f4-4097-af51-49b50d2b728b",
   "metadata": {
    "name": "MainCodeMD",
    "collapsed": false
   },
   "source": "## Main Code"
  },
  {
   "cell_type": "code",
   "id": "7d68cb81-c365-40fe-a5d3-af1aea7d0735",
   "metadata": {
    "language": "python",
    "name": "ConfigLoad",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def load_config(CONFIG_PATH=\"config.json\", RUN_NAME=\"PalantirTestRun\"):\n    with open(CONFIG_PATH) as f:\n        config = json.load(f)\n    config_run = next((r for r in config['runs'] if r['run_name'] == RUN_NAME), None)\n    return config, config_run\n\nconfig, config_run = load_config() #DEBUG\nprint(config, config_run)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a31cbd32-6636-4312-9902-7e2169e1ab38",
   "metadata": {
    "language": "python",
    "name": "URLString",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def create_url_string(config_run, config):\n    calc_to = date.today()\n    calc_from = calc_to - timedelta(days=config_run[\"from_days_ago\"])\n    calc_from = calc_from.isoformat()\n    \n    url_string = f\"\"\"\n        {config_run[\"endpoint\"]}?\n        q={config_run[\"q\"]}&\n        language={config_run[\"language\"]}&\n        to={calc_to}&\n        from={calc_from}&\n        sortBy={config_run[\"sortBy\"]}&\n        apiKey={config[\"news_api_key\"]}\n    \"\"\"\n    url_string = url_string.replace(' ', '').replace('\\n', '')\n    return url_string\n\nurl_string = create_url_string(config_run, config)\nprint(url_string)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6feb7410-9596-47e3-b57f-deed0d937a43",
   "metadata": {
    "language": "python",
    "name": "ToFetchNews",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "def fetch_news(url_string):\n    \"\"\"\n    Fetches news, makes sures all the pages are scraped\n    \"\"\"\n    response = requests.get(url_string, params = {\"page\": 1})\n    data = response.json()\n\n    total_results = data.get(\"totalResults\", 0)\n    articles_data = data.get(\"articles\", [])\n    # print(f'total_results: {total_results} | articles_data: {articles_data}')\n    \n    if total_results == 0:\n        return articles_data\n        \n    if total_results > 100:\n        total_pages = math.ceil(total_results / 100)\n        for page in range(2, total_pages+1):\n            print(f'Going through page {page}/{total_pages}')\n            response = requests.get(url_string, params = {\"page\": page})\n            data = response.json()\n            \n            page_articles = data.get(\"articles\")\n            articles_data.extend(page_articles)\n            # print(page_articles)\n            \n    if len(articles_data) != total_results:\n        warnings.warn(f'Article Count Mismatch\" {len(articles_data)}|{total_results}')\n\n    print(f'No. of Articles: ')\n    return articles_data\n\nprint(url_string)\narticles_data = fetch_news(url_string)\nprint(articles_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "632b0a14-47c1-4204-aa07-dc2cb8872798",
   "metadata": {
    "language": "python",
    "name": "CreatinbBaseDF",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def articles_to_df(articles_data):\n    def get_domain(url: str) -> str:\n        \"\"\"Extract domain from URL (finance.yahoo.com, etc.).\"\"\"\n        if not url or pd.isna(url):\n            return None\n        parsed = urlparse(url)\n        return parsed.netloc.lower()\n        \n    df = pd.json_normalize(articles_data, sep=\"_\") #Flatten and separate by\n    column_rename_mapping = {\n        \"urlToImage\": \"url_to_image\",\n        \"publishedAt\": \"published_at\",\n        \"content\": \"content_truncated\"\n    }\n    df = df.rename(columns=column_rename_mapping)\n    df[\"url_domain\"] = df[\"url\"].apply(get_domain)\n    df[\"published_at\"] = pd.to_datetime(df[\"published_at\"], utc=True).dt.tz_localize(None)\n    return df\n\ndf = articles_to_df(articles_data)\ndf.head(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc8028a3-bb17-4926-a3c5-27f1f1145f8a",
   "metadata": {
    "language": "python",
    "name": "CreateNR",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def ensure_network_rule_for_domain(\n    df, \n    nr_name = \"news_domains_nr\", \n    integration_name = \"news_domains_integration\"):\n    \"\"\"\n    Create or replace a single Snowflake network rule that covers all given domains.\n    Attach it to the API integration.\n    \"\"\"\n    config, run_config  = load_config()\n    config_snowflake = config[\"snowflake\"]\n    conn = snowflake.connector.connect(\n        user=config_snowflake[\"user\"],\n        password=config_snowflake[\"password\"],\n        account=config_snowflake[\"account\"],\n        warehouse=config_snowflake[\"warehouse\"],\n        database=config_snowflake[\"database\"],\n        schema=\"UTILS\" # SQL Specific\n    )\n    cur = conn.cursor() #Start\n\n    cur.execute(f\"DESCRIBE NETWORK RULE {nr_name}\")\n    desc_rows = cur.fetchall()\n    desc_list = list(desc_rows[0])\n    \n    for idx, val in enumerate(desc_list):\n        if \".com\" in str(val).lower():\n            value_list_str_existing = val.lower()\n    print(value_list_str_existing)\n            \n    value_list_existing = [d.strip().strip(\"'\").lower() for d in value_list_str_existing.split(\",\")]\n    value_list_latest = [i.lower() for i in list(df['url_domain'].unique())]\n    value_list_optimised = list(set(value_list_existing + value_list_latest))\n    value_list_str_optimised = \", \".join([f\"'{d}'\" for d in value_list_optimised])\n    print(f'Existing Value List: {value_list_existing}')\n    print(f'Latest Value List: {value_list_latest}')\n    print(f'Optimised Value List: {value_list_optimised}')\n    print(f'NR String: {value_list_str_optimised}')\n\n    # Network Rule SQL\n    create_sql = f\"\"\"\n    CREATE OR REPLACE NETWORK RULE {nr_name}\n        TYPE = HOST_PORT\n        MODE = EGRESS\n        VALUE_LIST = ({value_list_str_optimised});\n    \"\"\"\n    cur.execute(create_sql)\n    \n    # Collect all network rules and re-attach to integration\n    alter_sql = f\"\"\"\n    ALTER EXTERNAL ACCESS INTEGRATION {integration_name}\n        SET ALLOWED_NETWORK_RULES = ({nr_name})\n        ENABLED = TRUE;\n    \"\"\"\n    cur.execute(alter_sql)\n    cur.close() #End\n    \n    print(f\"Network rule ({nr_name}) updated in Integration ({integration_name})\")\n\nensure_network_rule_for_domain(df)\n    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2dbe2028-464a-4404-bb46-10279c649b1f",
   "metadata": {
    "language": "python",
    "name": "Scrape",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def scrape_url_helper(url):\n    try:\n        response = requests.get(url, timeout=10)\n        # if response.status_code != 200:\n        #     return None, 0\n            \n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # title = soup.title.string.strip() if soup.title else \"\" # Extract title\n        paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')] # Extract all paragraph text\n        content = \" \".join(paragraphs)\n        \n        # paragraphs = soup.find_all(\"p\") #[GPT Method] Keeps formatting intact (other method, removes additional newlines and spaces)\n        # content = \" \".join([p.get_text() for p in paragraphs if p.get_text()])\n        # content = content.strip()\n        print(f'{url} - content')\n        return content, len(content)\n    except requests.exceptions.Timeout as e:\n        # Handle connection timeout specifically\n        print(f\"TIMEOUT for url {url}: {e}\")\n        return None, -2\n    except Exception as e:\n        # If error (broken link, paywall, etc.)\n        print(f\"ERROR for url {url}: {type(e).__name__} — {e}\")\n        return None, -1 #(-1 to indicate that domain network rule not created)\n\ndef scrape_url(df, chunk_size=30, delay_between_chunks=3):\n    df['content_full'] = None\n    df['content_size'] = 0\n    df_len = len(df)\n\n    for idx, row in df.iterrows():\n        url = row.get('url')\n        if not url:\n            continue\n        content_full, content_size = scrape_url_helper(url)\n        print(f'{idx+1}/{df_len} | Scraped url({content_size}): {url}')\n        df.at[idx, 'content_full'] = content_full\n        df.at[idx, 'content_size'] = content_size\n        if idx%chunk_size == 0:\n            print(f'{idx+1}/{df_len} | Sleeping for {delay_between_chunks}s')\n            time.sleep(delay_between_chunks)\n    return df\n    \n\ndf = scrape_url(df)\ndf.head(20)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35583064-8829-4510-b24f-230743eacd76",
   "metadata": {
    "name": "UpdatingSchemaMD",
    "collapsed": false
   },
   "source": "### Updating Schema View"
  },
  {
   "cell_type": "code",
   "id": "46c72de0-c6b8-449e-b669-3d29dd2dfcc2",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df[df[\"content_size\"] == -2]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d11bad1-e553-4337-a637-03325f5326e8",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}